{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "b8770cbb-b09b-4866-acfc-b07d6eaa0fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using {device}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "630ae434-35e7-44fb-a1da-4f7f872a63a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf8cc1b-e464-4bc1-b895-6704e54f8aa7",
   "metadata": {},
   "source": [
    "# Implementing Data Loader\n",
    "\n",
    "*  The CIFAR-10 dataset is composed of 60000 small (3 ×32×32) color images, each of which belongs to one of  10 classes\n",
    "*   I have resized the images in the dataset in $32 \\times 32$  images.\n",
    "\n",
    "* I have implemented few data augamentation techniques such as `RandomHorizontalFlip` and `ColorJitter` in order to avoid overfitting..\n",
    "\n",
    "* The following function is used to load the dataset and resize the images\n",
    "\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "4061a49a-3575-4fd1-b4ec-dc3ab258947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "def load_data_CIFAR(batch_size, resize):\n",
    "    \"\"\"Download the CIFAR-10 dataset and then load it into memory.\"\"\"\n",
    "    trans = [torchvision.transforms.ToTensor()]\n",
    "    # Randomly flip the image horizontally\n",
    "    trans.append(transforms.RandomHorizontalFlip(p=0.6))\n",
    "\n",
    "    # Apply Colour Gitter\n",
    "    trans.append(transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1))\n",
    "    \n",
    "    if resize:\n",
    "        trans.insert(0, torchvision.transforms.Resize(resize))\n",
    "    trans = torchvision.transforms.Compose(trans)\n",
    "\n",
    "    mnist_train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=trans)\n",
    "    mnist_test = torchvision.datasets.CIFAR10(root=\"../data\", train=False, transform=trans, download=True)\n",
    "\n",
    "    return (torch.utils.data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=2, pin_memory=True), # Using pinned memory\n",
    "            torch.utils.data.DataLoader(mnist_test, batch_size, shuffle=False, num_workers=2, pin_memory=True)) # Using pinned memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "894676c2-0af0-4d98-9a30-bde00ca7ddea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64 # Defines the batch size\n",
    "train_iter, test_iter = load_data_CIFAR(batch_size, resize=(32, 32)) #Load train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "29bb2ae8-2bc9-4dc6-a1cd-5337edf44e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "X, y = next(iter(train_iter)) # Requests the first training batch\n",
    "print(X.size()) # 64 images per batch. Each image is represented by a 3 x 32 x 32 tensor (number of channels x height x width). The images are RGB, so there are 3 channels.\n",
    "print(y.size()) # 64 targets. Each target is a number between 0 and 9. The classification problem has 10 clases.\n",
    "torch.cuda.empty_cache() #Empty cache for improving performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b38a936-9d7f-4863-a16e-fc1660b7a9be",
   "metadata": {},
   "source": [
    "**Task 2:  Implementing a neural network based on the architecture described in Basic Architecture**\n",
    "\n",
    "\n",
    "**1) Intermediate Block**\n",
    "\n",
    "\n",
    "* The constructor for `Intermediate block` receives the number of convolutional layer'$c_n$', number of output channels $c_o$, the number of input channels $c_i$ and the dropout rate  $d_r$.\n",
    "\n",
    "* Following is the step by step implementation of the intermediate block\n",
    "    1. The input image goes through $c_n$ independent convolutional layers with $c_o$ convolutional filters, each with a $6 \\times 6$ window and padding $2$.\n",
    "    2. The resulting image of each convolutional layer goes through a max_pooling with window size $3 \\times 3$, stride of $2$ and padding of $1$\n",
    "    3. The resulting image goes through batch normalization layer for images and a rectified linear activation function.\n",
    "    4. The resulting image goes through a dropout function to introduce normalization with the dropout rate $d_r$.\n",
    "    5. The resulting image goes is multiplied with a weighted vector `$a$` which is obtained by a linear layer and `sigmoid` activation, the linear layer takes the input vector `$m$` which is the average of the each channel of input image.\n",
    "    6. We then perform the sum of the above computation to obtain the final resulting image of a single intermediate block.\n",
    "    7. The resulting image is then passed through `L` independent blocks going through same process defined in `steps 1-5`.\n",
    "    8. The Resulting image from the last intermediate block is then passed to the output block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "acd064b8-de6a-4b9f-ab04-b38839697044",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntermediateBlock(nn.Module):\n",
    "    def __init__(self, num_layers, num_output_channels, input_channels,dropout_rate):\n",
    "        super(IntermediateBlock, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.num_output_channels = num_output_channels\n",
    "        \n",
    "        # Define convolutional layers\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.relu_activations = nn.ModuleList()\n",
    "        self.channel_max = nn.ModuleList()\n",
    "        self.dropouts = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv_layer = nn.Conv2d(input_channels, num_output_channels, kernel_size=6, padding=2)\n",
    "            self.conv_layers.append(conv_layer)\n",
    "            bn = torch.nn.BatchNorm2d(num_output_channels)\n",
    "            self.batch_norms.append(bn)\n",
    "            relu = torch.nn.ReLU()\n",
    "            self.relu_activations.append(relu)\n",
    "            max_pool = torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "            self.channel_max.append(max_pool)\n",
    "            dropout = torch.nn.Dropout(p=dropout_rate)\n",
    "            self.dropouts.append(dropout)\n",
    "        \n",
    "        # Define fully connected layer for computing coefficients\n",
    "        self.fc = nn.Linear(input_channels, num_layers)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layers\n",
    "        conv_outputs = []\n",
    "        for conv_layer,bn,relu,dropout,max_pool in zip(self.conv_layers,self.batch_norms,self.relu_activations,self.dropouts,self.channel_max):\n",
    "            conv_output = conv_layer(x)\n",
    "            conv_output = max_pool(conv_output)\n",
    "            conv_output = bn(conv_output)\n",
    "            conv_output = relu(conv_output)\n",
    "            conv_output = dropout(conv_output)\n",
    "            conv_outputs.append(conv_output)\n",
    "        \n",
    "        # Compute average values for each channel\n",
    "        m = torch.mean(x, dim=(2, 3))\n",
    "        \n",
    "        # Compute coefficients using fully connected layer\n",
    "        a = (self.fc(m))\n",
    "        a = self.sigmoid(a)\n",
    "        \n",
    "        # Reshape coefficients to match convolutional outputs\n",
    "        a = a.view(a.size(0), -1, 1, 1, 1)  # Reshape to (batch_size, num_layers, 1, 1)\n",
    "        \n",
    "        # Combine outputs with coefficients\n",
    "        weighted_outputs = [a[:, i] * conv_outputs[i] for i in range(self.num_layers)]\n",
    "        \n",
    "        # Sum the weighted outputs\n",
    "        x_prime = sum(weighted_outputs)\n",
    "        \n",
    "        return x_prime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ff9c17-9611-4175-8964-e19695289b4b",
   "metadata": {},
   "source": [
    "## Output Block\n",
    "\n",
    "* The constructor for `OutputBlock` receives the number of input channels $c_i$ and the number of classes $c_c$.\n",
    "\n",
    "* Following is the step by step implementation of the output block:\n",
    "    1. First the mean of the each channel of the input image is calculated as vector `$m$`.\n",
    "    2. The image that results from step 1 is passed to a fully connected linear layer which gives out the `logits ($o$)` for each class.\n",
    "    3. The resulting logits which is the probability distribution of each class in our dataset is then returned as the final output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "e5b75965-fcb5-4f4c-afa4-0845b9c0d912",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputBlock(nn.Module):\n",
    "    def __init__(self, num_channels, num_classes):\n",
    "        super(OutputBlock, self).__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.fc_layers = nn.Linear(num_channels, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute average values for each channel\n",
    "        m = torch.mean(x, dim=(2, 3))  # Global average pooling\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "        o = self.fc_layers(m)\n",
    "        \n",
    "        return o\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e776d4-190f-4897-97f2-fecbd74d5e6d",
   "metadata": {},
   "source": [
    "## BasicNet\n",
    "\n",
    "* The constructor for `BasicNet` Defines the number of intermediate and output block.\n",
    "\n",
    "* Following is the step by step implementation of the BasicNet class:\n",
    "    1. First all the intermediate blocks are defined by passing the required parameters as, this allows to create the blocks with different convolutional layers as well as different parameters such as dropout rates `$d_r$`.\n",
    "    2. The number of input channels $c_i$ for the next block is equal to the number of output channels $c_o$ for the previous layer.\n",
    "    3. The number of convolutional layers for blocks `1,2 and 3` are `4,3,2` respectively\n",
    "    4. Finally the output block is defined which will give the logits as the final output.\n",
    "    5. The resulting logits which is the probability distribution of each class in our dataset is then returned as the final output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "5d874170-0434-4feb-ae69-0c352769774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicNet, self).__init__()\n",
    "        self.intermediate_block1 = IntermediateBlock(num_layers=4, num_output_channels=128, input_channels=3,dropout_rate=0.5)\n",
    "        self.intermediate_block2 = IntermediateBlock(num_layers=3, num_output_channels=64, input_channels=128,dropout_rate=0.5)\n",
    "        self.intermediate_block3 = IntermediateBlock(num_layers=2, num_output_channels=32, input_channels=64,dropout_rate=0.5)\n",
    "        self.output_block = OutputBlock(num_channels=32, num_classes=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.intermediate_block1(x)\n",
    "        x = self.intermediate_block2(x)\n",
    "        x = self.intermediate_block3(x)\n",
    "        # x = self.intermediate_block4(x)\n",
    "        x = self.output_block(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "44e3c0fd-52bb-4c03-88cf-7fc6e66152da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies Xavier initialization if the `torch.nn.Module` is `torch.nn.Linear` or `torch.nn.Conv2d`\n",
    "def init_weights(m):\n",
    "    if type(m) == torch.nn.Linear or type(m) == torch.nn.Conv2d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee43d86-ba16-4118-8b59-6b196d79e9ff",
   "metadata": {},
   "source": [
    "**Below I have moved the model to GPU and defined its object and applied weight initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "1cb4883d-7b4d-48ba-9bc8-de9b56d600d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BasicNet(\n",
       "  (intermediate_block1): IntermediateBlock(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0-3): 4 x Conv2d(3, 128, kernel_size=(6, 6), stride=(1, 1), padding=(2, 2))\n",
       "    )\n",
       "    (batch_norms): ModuleList(\n",
       "      (0-3): 4 x BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (relu_activations): ModuleList(\n",
       "      (0-3): 4 x ReLU()\n",
       "    )\n",
       "    (channel_max): ModuleList(\n",
       "      (0-3): 4 x MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (dropouts): ModuleList(\n",
       "      (0-3): 4 x Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (fc): Linear(in_features=3, out_features=4, bias=True)\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       "  (intermediate_block2): IntermediateBlock(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0-2): 3 x Conv2d(128, 64, kernel_size=(6, 6), stride=(1, 1), padding=(2, 2))\n",
       "    )\n",
       "    (batch_norms): ModuleList(\n",
       "      (0-2): 3 x BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (relu_activations): ModuleList(\n",
       "      (0-2): 3 x ReLU()\n",
       "    )\n",
       "    (channel_max): ModuleList(\n",
       "      (0-2): 3 x MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (dropouts): ModuleList(\n",
       "      (0-2): 3 x Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (fc): Linear(in_features=128, out_features=3, bias=True)\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       "  (intermediate_block3): IntermediateBlock(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0-1): 2 x Conv2d(64, 32, kernel_size=(6, 6), stride=(1, 1), padding=(2, 2))\n",
       "    )\n",
       "    (batch_norms): ModuleList(\n",
       "      (0-1): 2 x BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (relu_activations): ModuleList(\n",
       "      (0-1): 2 x ReLU()\n",
       "    )\n",
       "    (channel_max): ModuleList(\n",
       "      (0-1): 2 x MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (dropouts): ModuleList(\n",
       "      (0-1): 2 x Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (fc): Linear(in_features=64, out_features=2, bias=True)\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       "  (output_block): OutputBlock(\n",
       "    (fc_layers): Linear(in_features=32, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Set up training parameters\n",
    "net = BasicNet().to(device)\n",
    "net.apply(init_weights) # Applies `init_weights` to every `torch.nn.Module` inside `model`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e57597-8fec-41fc-bd37-a14bc457c4b1",
   "metadata": {},
   "source": [
    "## Loss function, optimizer and scheduler\n",
    "\n",
    "* The *convolutional neural network* defined above computes the logits matrix $\\mathbf{O}$.\n",
    "\n",
    "* This is because PyTorch provides a class called `CrossEntropyLoss` that implements the desired cross entropy loss but requires a logits matrix $\\mathbf{O}$ instead of the prediction matrix $\\mathbf{\\hat{Y}}$.\n",
    "\n",
    "* The class `CrossEntropyLoss` implements the cross entropy loss in a way that avoids numerical instabilities that would result from a naive implementation.\n",
    "* I have applied `Adam` optimizer with learning rate of `0.001`\n",
    "* I have used `StepLR` scheduler with step size of `$15$` and gamma = `$0.9$` which will multiply my learning rate with `$0.9$` after `$15$` epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "aea2a07f-5fe1-4996-9400-15f9e0f56e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=15, gamma=0.9)  # Adjust the learning rate every 20 epochs by multiplying it with 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46483a6-8227-4e58-9d89-cdea628e7704",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "* Since the highest element of a logits vector determines which class will be predicted.\n",
    "\n",
    "* We can use this to compute the number of correct predictions per batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "cf03a5b4-f2c7-42f3-92bb-0f4a805bcfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(logits, y):\n",
    "    y_hat = logits.argmax(axis=1)  # Finds the column with the highest value for each row of `logits`.\n",
    "    return (y_hat == y).float().sum()  # Computes the number of times that `y_hat` and `y` match."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407e73bd-977c-4a19-ac8d-b49003a5e300",
   "metadata": {},
   "source": [
    "* We can use the previous function to compute the accuracy of our model in a given dataset by accumulating the number of correct predictions across batches and then dividing that number by the number of examples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "343b629c-4bcf-4cf2-a423-a8df45e66a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to evaluate metrics\n",
    "def evaluate_metric(model, data_iter, metric):\n",
    "    total_metric = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            logits = model(X)\n",
    "            total_metric += metric(logits, y).item()\n",
    "            total_samples += y.size(0)\n",
    "    return total_metric / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "eba77864-8234-4a4c-a7af-2975488dfc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.1. Testing accuracy: 0.1.\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "print(f'Training accuracy: {evaluate_metric(net, train_iter, correct)}. Testing accuracy: {evaluate_metric(net, test_iter, correct)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a8ad7f-866c-4381-98bf-70df97d30553",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "* The following code implements the training loop for the convolutional neural network.\n",
    "\n",
    "* The training/testing dataset accuracy is displayed after each epoch and stored for plotting.\n",
    "* The training and test accuracies as well as the cross entropy loss for each batch are then plotted at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4662f819-5263-4168-be89-77ae58c0ed7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/80.\n",
      "Training accuracy: 0.94916. Testing accuracy: 0.8588. Duration: 67.439s.\n",
      "\n",
      "Epoch 2/80.\n",
      "Training accuracy: 0.95016. Testing accuracy: 0.8547. Duration: 68.659s.\n",
      "\n",
      "Epoch 3/80.\n",
      "Training accuracy: 0.95304. Testing accuracy: 0.8534. Duration: 67.835s.\n",
      "\n",
      "Epoch 4/80.\n",
      "Training accuracy: 0.95274. Testing accuracy: 0.8563. Duration: 68.154s.\n",
      "\n",
      "Epoch 5/80.\n",
      "Training accuracy: 0.94542. Testing accuracy: 0.847. Duration: 69.310s.\n",
      "\n",
      "Epoch 6/80.\n",
      "Training accuracy: 0.96114. Testing accuracy: 0.8591. Duration: 67.215s.\n",
      "\n",
      "Epoch 7/80.\n",
      "Training accuracy: 0.96148. Testing accuracy: 0.8614. Duration: 66.594s.\n",
      "\n",
      "Epoch 8/80.\n",
      "Training accuracy: 0.95016. Testing accuracy: 0.8517. Duration: 67.277s.\n",
      "\n",
      "Epoch 9/80.\n",
      "Training accuracy: 0.96086. Testing accuracy: 0.8583. Duration: 67.838s.\n",
      "\n",
      "Epoch 10/80.\n",
      "Training accuracy: 0.9504. Testing accuracy: 0.8575. Duration: 66.320s.\n",
      "\n",
      "Epoch 11/80.\n",
      "Training accuracy: 0.9649. Testing accuracy: 0.8649. Duration: 66.103s.\n",
      "\n",
      "Epoch 12/80.\n",
      "Training accuracy: 0.96232. Testing accuracy: 0.8575. Duration: 65.761s.\n",
      "\n",
      "Epoch 13/80.\n",
      "Training accuracy: 0.963. Testing accuracy: 0.8595. Duration: 66.420s.\n",
      "\n",
      "Epoch 14/80.\n",
      "Training accuracy: 0.96476. Testing accuracy: 0.8632. Duration: 65.782s.\n",
      "\n",
      "Epoch 15/80.\n",
      "Training accuracy: 0.95064. Testing accuracy: 0.8493. Duration: 65.834s.\n",
      "\n",
      "Epoch 16/80.\n",
      "Training accuracy: 0.95494. Testing accuracy: 0.8556. Duration: 65.234s.\n",
      "\n",
      "Epoch 17/80.\n",
      "Training accuracy: 0.96394. Testing accuracy: 0.8632. Duration: 66.350s.\n",
      "\n",
      "Epoch 18/80.\n",
      "Training accuracy: 0.96534. Testing accuracy: 0.8595. Duration: 65.451s.\n",
      "\n",
      "Epoch 19/80.\n",
      "Training accuracy: 0.96086. Testing accuracy: 0.8599. Duration: 66.046s.\n",
      "\n",
      "Epoch 20/80.\n",
      "Training accuracy: 0.96574. Testing accuracy: 0.8644. Duration: 64.798s.\n",
      "\n",
      "Epoch 21/80.\n",
      "Training accuracy: 0.96732. Testing accuracy: 0.8648. Duration: 65.173s.\n",
      "\n",
      "Epoch 22/80.\n",
      "Training accuracy: 0.95522. Testing accuracy: 0.85. Duration: 66.375s.\n",
      "\n",
      "Epoch 23/80.\n"
     ]
    }
   ],
   "source": [
    "losses = [] # Stores the loss for each training batch\n",
    "train_accs = [] # Stores the training accuracy after each epoch\n",
    "test_accs = [] # Stores the testing accuracy after each epoch\n",
    "\n",
    "num_epochs = 80\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'\\nEpoch {epoch + 1}/{num_epochs}.')\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    net.train() # This is necessary because batch normalization behaves differently between training and evaluation\n",
    "\n",
    "    for X, y in train_iter:\n",
    "        X, y = X.to(device), y.to(device) # Moves data to `device`\n",
    "        logits = net(X) # Computes the logits for the batch of images `X`\n",
    "\n",
    "        l = loss_fn(logits, y) # Computes the loss given the `logits` and the class vector `y`\n",
    "        optimizer.zero_grad() # Zeroes the gradients stored in the model parameters\n",
    "        l.backward() # Computes the gradient of the loss `l` with respect to the model parameters\n",
    "\n",
    "        optimizer.step() # Updates the model parameters based on the gradients stored inside them\n",
    "        losses.append(float(l)) # Stores the loss for this batch\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    with torch.no_grad(): # Computing performance metrics does not require gradients\n",
    "        net.eval() # This is necessary because batch normalization behaves differently between training and evaluation\n",
    "        train_accs.append(evaluate_metric(net, train_iter, correct))\n",
    "        test_accs.append(evaluate_metric(net, test_iter, correct))\n",
    "\n",
    "        end_time = time.perf_counter()\n",
    "\n",
    "        print(f'Training accuracy: {train_accs[-1]}. Testing accuracy: {test_accs[-1]}. Duration: {end_time - start_time:.3f}s.') # Computes and displays training/testing dataset accuracy.\n",
    "        torch.cuda.empty_cache() #Empty cache for improving performance\n",
    "\n",
    "plt.plot(losses) # Plots the loss for each training batch\n",
    "plt.xlabel('Training batch')\n",
    "plt.ylabel('Cross entropy loss')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_accs, label='Training accuracy')\n",
    "plt.plot(test_accs, label='Testing accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3bece6-eae1-4950-9aea-5747089adadc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7516ff8a-ae67-4686-af38-d5b3dd19266b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
